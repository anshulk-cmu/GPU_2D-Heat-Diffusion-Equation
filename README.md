# GPU-Accelerated 2D Heat Diffusion Equation Solver

A high-performance parallel implementation of the 2D heat diffusion equation solver using CUDA (CuPy) and Python, demonstrating up to **51√ó speedup** over CPU implementations on NVIDIA A100 GPU.

## Overview

This project implements numerical solvers for the 2D heat diffusion equation using both CPU (NumPy) and GPU (CuPy/CUDA) backends. The solver uses finite difference methods with explicit (Forward Euler) and implicit (Jacobi iteration) time-stepping schemes to solve parabolic PDEs governing thermal diffusion processes.

## Mathematical Foundation

**Governing Equation:**
```
‚àÇu/‚àÇt = Œ±(‚àÇ¬≤u/‚àÇx¬≤ + ‚àÇ¬≤u/‚àÇy¬≤)
```

Where:
- `u(x,y,t)`: Temperature distribution at position (x,y) and time t
- `Œ± = 0.01 m¬≤/s`: Thermal diffusivity coefficient
- Domain: [0,1] √ó [0,1] meters

**Discretization:**
- **Spatial**: 5-point stencil with central differences (2nd-order accurate)
- **Temporal**: Forward Euler (explicit) and Backward Euler with Jacobi iterations (implicit)
- **Stability**: CFL condition enforced with r = 0.20 (Œît ‚â§ h¬≤/4Œ±)
- **Boundary Conditions**: Dirichlet (0K on all edges)

## Implementation Features

### CPU Implementation (NumPy)
- Vectorized operations for efficient memory access
- Explicit scheme: Direct time-stepping
- Jacobi solver: Iterative convergence (Œµ = 10‚Åª‚Å∂, max 10,000 iterations)

### GPU Implementation (CUDA via CuPy)

**Two optimization variants implemented:**

1. **Basic Global Memory Kernels**
   - Direct global memory access with coalesced reads/writes
   - Grid-stride loop structure for arbitrary problem sizes
   - 16√ó16 thread blocks (256 threads per block)

2. **Shared Memory Optimization**
   - 18√ó18 shared memory tiles for data reuse
   - Halo exchange for ghost cells at block boundaries
   - Reduced global memory traffic by ~3√ó

**Thread Configuration:**
- Block size: 16√ó16 = 256 threads
- Warps per block: 8
- Theoretical occupancy: 50%

## Hardware Specifications

**NVIDIA A100-SXM4-80GB (Ampere Architecture)**
- Compute Capability: 8.0
- Memory: 85.17 GB HBM2e
- Multiprocessors: 108 SMs
- Memory Bandwidth: ~2048 GB/s (peak)
- L2 Cache: 41.94 MB
- Clock Rate: 1.41 GHz
- Peak Performance: ~19.5 TFLOPS (FP64)

## Performance Results

### CPU Baseline (NumPy)

| Grid Size | Explicit (100 steps) | Jacobi (10 steps) | Iterations |
|-----------|---------------------|-------------------|------------|
| 256√ó256   | 0.0742 s | 6.9930 s | 10,000 |
| 512√ó512   | 0.3062 s | 20.4723 s | 10,000 |
| 1024√ó1024 | 1.9353 s | 97.7565 s | 10,000 |

### GPU Performance & Speedup

#### 256√ó256 Grid
- **Basic Explicit**: 0.077 s ‚Üí **0.96√ó speedup**
- **Shared Explicit**: 0.087 s ‚Üí 0.85√ó speedup
- **Basic Jacobi**: 3.368 s ‚Üí **2.08√ó speedup** (10,000 iterations)
- **Shared Jacobi**: 3.399 s ‚Üí 2.06√ó speedup

#### 512√ó512 Grid
- **Basic Explicit**: 0.039 s ‚Üí **7.85√ó speedup**
- **Shared Explicit**: 0.039 s ‚Üí **7.89√ó speedup**
- **Basic Jacobi**: 3.676 s ‚Üí **5.57√ó speedup** (10,000 iterations)
- **Shared Jacobi**: 3.641 s ‚Üí **5.62√ó speedup**

#### 1024√ó1024 Grid (Best Performance)
- **Basic Explicit**: 0.038 s ‚Üí **50.64√ó speedup** ‚ö°
- **Shared Explicit**: 0.038 s ‚Üí **51.07√ó speedup** ‚ö°
- **Basic Jacobi**: 3.774 s ‚Üí **25.90√ó speedup** (10,000 iterations)
- **Shared Jacobi**: 3.710 s ‚Üí **26.35√ó speedup**

### Memory Bandwidth Efficiency (2048√ó2048 Grid)

| Metric | Value | Peak Capability | Efficiency |
|--------|-------|-----------------|------------|
| Memory Bandwidth | 452.47 GB/s | 2048 GB/s | 22.2% |
| Compute Throughput | 103.69 GFLOPS | ~5000 GFLOPS | 2.1% |
| Kernel Execution Time | 44.4 ms | ‚Äî | ‚Äî |

*Note: Stencil computations are memory-bandwidth limited, not compute-limited*

## Numerical Validation

### GPU-CPU Consistency (50 time steps)

| Grid Size | L2 Error | L‚àû Error | Relative L2 | Status |
|-----------|----------|----------|-------------|--------|
| 128√ó128 | 5.99e-16 | 2.13e-14 | 1.35e-16 | ‚úÖ PASSED |
| 256√ó256 | 6.95e-16 | 2.84e-14 | 1.24e-16 | ‚úÖ PASSED |
| 512√ó512 | 4.57e-16 | 2.84e-14 | 7.52e-17 | ‚úÖ PASSED |

**Energy Conservation (256√ó256, 500 steps):**
- Initial Energy: 5.053319e+04
- Final Energy: 5.053319e+04
- Change: **0.0000%** ‚úÖ EXCELLENT

## Key Findings

### Performance Insights
1. **Dramatic scaling with problem size**: 0.96√ó ‚Üí 51√ó speedup as grid increases
2. **Shared memory provides minimal benefit** (~1% improvement), indicating memory bandwidth is not the primary bottleneck at these scales
3. **Jacobi method benefits significantly** from GPU parallelization (2-26√ó speedup)
4. **Memory bandwidth utilization** scales from 0.5% (256¬≤) to 22.2% (2048¬≤)
5. **GPU overhead dominates** at small grid sizes, causing CPU-parity or slower performance

### Technical Achievements
- ‚úÖ Machine precision accuracy (errors < 1e-14)
- ‚úÖ Perfect energy conservation
- ‚úÖ Robust numerical stability
- ‚úÖ Validated against analytical solutions
- ‚úÖ Production-ready CUDA kernels

## Project Structure

```
2D_Heat_Diffusion_Equation_Solver.ipynb
‚îú‚îÄ‚îÄ Installation & GPU Verification
‚îú‚îÄ‚îÄ Mathematical Foundations
‚îÇ   ‚îú‚îÄ‚îÄ Governing equations
‚îÇ   ‚îú‚îÄ‚îÄ Finite difference discretization
‚îÇ   ‚îú‚îÄ‚îÄ Stability analysis (CFL condition)
‚îÇ   ‚îî‚îÄ‚îÄ Numerical parameters
‚îú‚îÄ‚îÄ CPU Implementation
‚îÇ   ‚îú‚îÄ‚îÄ Explicit Forward Euler solver
‚îÇ   ‚îú‚îÄ‚îÄ Jacobi iterative solver
‚îÇ   ‚îî‚îÄ‚îÄ Performance benchmarking
‚îú‚îÄ‚îÄ GPU Implementation
‚îÇ   ‚îú‚îÄ‚îÄ Basic CUDA kernels
‚îÇ   ‚îú‚îÄ‚îÄ Shared memory optimization
‚îÇ   ‚îî‚îÄ‚îÄ Performance profiling
‚îú‚îÄ‚îÄ Numerical Validation
‚îÇ   ‚îú‚îÄ‚îÄ CPU-GPU consistency checks
‚îÇ   ‚îú‚îÄ‚îÄ Energy conservation tests
‚îÇ   ‚îú‚îÄ‚îÄ Convergence analysis
‚îÇ   ‚îî‚îÄ‚îÄ Manufactured solution validation
‚îî‚îÄ‚îÄ Visualizations
    ‚îú‚îÄ‚îÄ Temperature evolution animations
    ‚îú‚îÄ‚îÄ Performance scaling plots
    ‚îî‚îÄ‚îÄ Error analysis charts
```

## Usage

**Open in Google Colab:**

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/anshulk-cmu/GPU_2D-Heat-Diffusion-Equation/blob/main/2D_Heat_Diffusion_Equation_Solver.ipynb)

**The notebook includes:**
- Complete mathematical derivations with theory
- CPU and GPU implementation with detailed comments
- Comprehensive benchmarking suite
- Numerical validation and error analysis
- Interactive visualizations of heat diffusion
- Performance profiling and optimization analysis

## Requirements

```bash
cupy-cuda12x      # GPU acceleration
numpy             # CPU computation
matplotlib        # Plotting
scikit-image      # Image processing
scipy             # Scientific computing
```

**Hardware Requirements:**
- NVIDIA GPU with CUDA compute capability ‚â• 3.5
- 2-8 GB GPU memory (depending on grid size)
- CUDA Toolkit 12.x

## Applications

- üî• **Thermal Analysis**: Heat transfer in materials and electronics
- üß™ **Materials Science**: Diffusion processes in solids and fluids
- üñºÔ∏è **Image Processing**: Gaussian blur, denoising, edge detection
- üíπ **Financial Modeling**: Black-Scholes PDE, option pricing
- üåä **Fluid Dynamics**: Viscous flow, concentration diffusion
- üéì **Education**: Teaching numerical methods and GPU programming

## Citation

If you use this code in your research, please cite:

```bibtex
@software{gpu_heat_diffusion_2025,
  title={GPU-Accelerated 2D Heat Diffusion Equation Solver},
  author={Anshul Kumar},
  year={2025},
  url={https://github.com/anshulk-cmu/GPU_2D-Heat-Diffusion-Equation}
}
```

## License

MIT License - See LICENSE file for details

---

**GPU**: NVIDIA A100-SXM4-80GB | **Framework**: CuPy 13.x | **Python**: 3.10+ | **CUDA**: 12.x
