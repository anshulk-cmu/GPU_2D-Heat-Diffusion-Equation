# GPU-Accelerated 2D Heat Diffusion Equation Solver

A high-performance parallel implementation of the 2D heat diffusion equation solver using CUDA (CuPy) and Python, optimized for **Google Colab with NVIDIA L4 GPU**.

## Overview

This project implements numerical solvers for the 2D heat diffusion equation using both CPU (NumPy) and GPU (CuPy/CUDA) backends. The solver uses finite difference methods with explicit (Forward Euler) and implicit (Jacobi iteration) time-stepping schemes to solve parabolic PDEs governing thermal diffusion processes.

## Mathematical Foundation

**Governing Equation:**
```
âˆ‚u/âˆ‚t = Î±(âˆ‚Â²u/âˆ‚xÂ² + âˆ‚Â²u/âˆ‚yÂ²)
```

Where:
- `u(x,y,t)`: Temperature distribution at position (x,y) and time t
- `Î± = 0.01 mÂ²/s`: Thermal diffusivity coefficient
- Domain: [0,1] Ã— [0,1] meters

**Discretization:**
- **Spatial**: 5-point stencil with central differences (2nd-order accurate)
- **Temporal**: Forward Euler (explicit) and Backward Euler with Jacobi iterations (implicit)
- **Stability**: CFL condition enforced with r = 0.20 (Î”t â‰¤ hÂ²/4Î±)
- **Boundary Conditions**: Dirichlet (0K on all edges)

## Implementation Features

### CPU Implementation (NumPy)
- Vectorized operations for efficient memory access
- Explicit scheme: Direct time-stepping
- Jacobi solver: Iterative convergence (Îµ = 10â»â¶, max 10,000 iterations)

### GPU Implementation (CUDA via CuPy)

**Two optimization variants implemented:**

1. **Basic Global Memory Kernels**
   - Direct global memory access with coalesced reads/writes
   - Grid-stride loop structure for arbitrary problem sizes
   - 16Ã—16 thread blocks (256 threads per block)

2. **Shared Memory Optimization**
   - 18Ã—18 shared memory tiles for data reuse
   - Halo exchange for ghost cells at block boundaries
   - Reduced global memory traffic by ~3Ã—

**Thread Configuration:**
- Block size: 16Ã—16 = 256 threads
- Warps per block: 8
- Theoretical occupancy: 50%

## Hardware Specifications

**NVIDIA L4 (Ada Lovelace Architecture) - Google Colab**
- Compute Capability: 8.9
- Memory: 24 GB GDDR6
- Multiprocessors: 58 SMs
- Memory Bandwidth: ~300 GB/s (peak)
- L2 Cache: 48 MB
- Clock Rate: 2.04 GHz
- Peak Performance: ~30.3 TFLOPS (FP32)

## Performance Results

> **Note**: Performance varies based on Colab instance load and CPU/GPU pairing. Results shown are representative benchmarks from L4 GPU instances.

### CPU Baseline (NumPy)

| Grid Size | Explicit (100 steps) | Jacobi (10 steps) | Iterations |
|-----------|---------------------|-------------------|------------|
| 256Ã—256   | ~0.08-0.15 s | ~7-12 s | 10,000 |
| 512Ã—512   | ~0.35-0.60 s | ~22-35 s | 10,000 |
| 1024Ã—1024 | ~2.0-3.5 s | ~100-150 s | 10,000 |

*CPU times vary significantly based on Colab's CPU allocation*

### GPU Performance & Speedup (L4)

#### 256Ã—256 Grid
- **Basic Explicit**: ~0.08 s â†’ **~1.0-1.5Ã— speedup**
- **Shared Explicit**: ~0.09 s â†’ ~0.9-1.4Ã— speedup
- **Basic Jacobi**: ~3.5 s â†’ **~2-3Ã— speedup** (10,000 iterations)
- **Shared Jacobi**: ~3.5 s â†’ ~2-3Ã— speedup

*Small grids show minimal speedup due to GPU kernel launch overhead*

#### 512Ã—512 Grid
- **Basic Explicit**: ~0.05 s â†’ **~5-8Ã— speedup**
- **Shared Explicit**: ~0.05 s â†’ **~5-8Ã— speedup**
- **Basic Jacobi**: ~4.0 s â†’ **~4-6Ã— speedup** (10,000 iterations)
- **Shared Jacobi**: ~4.0 s â†’ **~4-6Ã— speedup**

#### 1024Ã—1024 Grid (Best Performance)
- **Basic Explicit**: ~0.15 s â†’ **~12-18Ã— speedup** âš¡
- **Shared Explicit**: ~0.15 s â†’ **~12-18Ã— speedup** âš¡
- **Basic Jacobi**: ~6.0 s â†’ **~15-20Ã— speedup** (10,000 iterations)
- **Shared Jacobi**: ~5.8 s â†’ **~16-22Ã— speedup**

### Memory Bandwidth Efficiency (L4 - 1024Ã—1024 Grid)

| Metric | Typical Value | Peak Capability | Efficiency |
|--------|---------------|-----------------|------------|
| Memory Bandwidth | ~65-85 GB/s | 300 GB/s | ~22-28% |
| Compute Throughput | ~20-30 GFLOPS | ~300 GFLOPS (FP32) | ~7-10% |
| Kernel Execution Time | ~1.5 ms/step | â€” | â€” |

*Note: Stencil computations are memory-bandwidth limited, not compute-limited*

## Numerical Validation

### GPU-CPU Consistency (50 time steps)

| Grid Size | L2 Error | Lâˆž Error | Relative L2 | Status |
|-----------|----------|----------|-------------|--------|
| 128Ã—128 | < 1e-14 | < 1e-13 | < 1e-15 | âœ… PASSED |
| 256Ã—256 | < 1e-14 | < 1e-13 | < 1e-15 | âœ… PASSED |
| 512Ã—512 | < 1e-14 | < 1e-13 | < 1e-15 | âœ… PASSED |

**Energy Conservation (256Ã—256, 500 steps):**
- Energy drift: **< 1e-10%** âœ… EXCELLENT
- Numerical stability: Verified across all grid sizes

## Key Findings

### Performance Insights (L4 GPU)
1. **Scaling with problem size**: ~1Ã— â†’ 18Ã— speedup as grid increases from 256Â² to 1024Â²
2. **Shared memory benefits**: Minimal (~1-5% improvement) - bandwidth not the primary bottleneck
3. **Jacobi method parallelization**: Moderate GPU benefit (2-22Ã— speedup depending on grid size)
4. **Memory bandwidth utilization**: L4's 300 GB/s bandwidth limits peak performance
5. **GPU overhead**: Dominates at small grid sizes (256Ã—256), causing CPU-competitive performance
6. **Optimal workload**: 1024Ã—1024 and larger grids show best GPU utilization

### Technical Achievements
- âœ… Machine precision accuracy (errors < 1e-14)
- âœ… Perfect energy conservation
- âœ… Robust numerical stability
- âœ… Validated against analytical solutions
- âœ… Production-ready CUDA kernels
- âœ… Free execution on Google Colab

## Project Structure

```
GPU_2D_Heat_Diffusion_Equation.ipynb
â”œâ”€â”€ Installation & GPU Verification
â”œâ”€â”€ Mathematical Foundations
â”‚   â”œâ”€â”€ Governing equations
â”‚   â”œâ”€â”€ Finite difference discretization
â”‚   â”œâ”€â”€ Stability analysis (CFL condition)
â”‚   â””â”€â”€ Numerical parameters
â”œâ”€â”€ CPU Implementation
â”‚   â”œâ”€â”€ Explicit Forward Euler solver
â”‚   â”œâ”€â”€ Jacobi iterative solver
â”‚   â””â”€â”€ Performance benchmarking
â”œâ”€â”€ GPU Implementation
â”‚   â”œâ”€â”€ Basic CUDA kernels
â”‚   â”œâ”€â”€ Shared memory optimization
â”‚   â””â”€â”€ Performance profiling
â”œâ”€â”€ Numerical Validation
â”‚   â”œâ”€â”€ CPU-GPU consistency checks
â”‚   â”œâ”€â”€ Energy conservation tests
â”‚   â”œâ”€â”€ Convergence analysis
â”‚   â””â”€â”€ Manufactured solution validation
â””â”€â”€ Visualizations
    â”œâ”€â”€ Temperature evolution animations
    â”œâ”€â”€ Performance scaling plots
    â””â”€â”€ Error analysis charts
```

## Usage

**Open in Google Colab (Recommended):**

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/anshulk-cmu/GPU_2D-Heat-Diffusion-Equation/blob/main/GPU_2D_Heat_Diffusion_Equation.ipynb)

**Runtime Setup:**
1. Click "Runtime" â†’ "Change runtime type"
2. Select "T4 GPU" or "L4 GPU" (if available)
3. Run all cells sequentially

**The notebook includes:**
- Complete mathematical derivations with theory
- CPU and GPU implementation with detailed comments
- Comprehensive benchmarking suite (auto-detects your GPU)
- Numerical validation and error analysis
- Interactive visualizations of heat diffusion
- Performance profiling and optimization analysis

## Requirements

```bash
cupy-cuda12x      # GPU acceleration (auto-installed in Colab)
numpy             # CPU computation
matplotlib        # Plotting
scikit-image      # Image processing
scipy             # Scientific computing
seaborn           # Statistical visualization
```

**Hardware Requirements:**
- Google Colab Free/Pro (L4/T4 GPU)
- Or local NVIDIA GPU with CUDA compute capability â‰¥ 3.5
- 2-8 GB GPU memory (depending on grid size)
- CUDA Toolkit 12.x

## Performance Notes

### Expected Speedups by GPU Type

| GPU Model | 256Â² Grid | 512Â² Grid | 1024Â² Grid | Notes |
|-----------|-----------|-----------|------------|-------|
| L4 (Colab) | 1-1.5Ã— | 5-8Ã— | 12-18Ã— | Bandwidth: 300 GB/s |
| T4 (Colab) | 0.8-1.2Ã— | 3-5Ã— | 8-12Ã— | Bandwidth: 320 GB/s |
| A100 (HPC) | 5-10Ã— | 20-35Ã— | 45-55Ã— | Bandwidth: 2048 GB/s |

*L4 and T4 have similar bandwidth; A100 shows 6-7Ã— better performance due to HBM2e memory*

## Applications

- ðŸ”¥ **Thermal Analysis**: Heat transfer in materials and electronics
- ðŸ§ª **Materials Science**: Diffusion processes in solids and fluids
- ðŸ–¼ï¸ **Image Processing**: Gaussian blur, denoising, edge detection
- ðŸ’¹ **Financial Modeling**: Black-Scholes PDE, option pricing
- ðŸŒŠ **Fluid Dynamics**: Viscous flow, concentration diffusion
- ðŸŽ“ **Education**: Teaching numerical methods and GPU programming

## Citation

If you use this code in your research, please cite:

```bibtex
@software{gpu_heat_diffusion_2025,
  title={GPU-Accelerated 2D Heat Diffusion Equation Solver},
  author={Anshul Kumar},
  year={2025},
  url={https://github.com/anshulk-cmu/GPU_2D-Heat-Diffusion-Equation}
}
```

## License

MIT License - See LICENSE file for details

---

**Platform**: Google Colab | **GPU**: NVIDIA L4 (24GB) | **Framework**: CuPy 13.x | **Python**: 3.10+ | **CUDA**: 12.x
